\section{Network theory}

  \subsection{Types of networks}
    
    A set of vertices joined by edges is only the simplest type of network; there are many ways in which networks may be more complex than this. For instance, there may be more than one different type of vertex in a network, or more than one different type of edge. And vertices or edges may have a variety of properties, numerical or otherwise, associated with them. Taking the example of a social network of people, the vertices may represent men or women, people of different nationalities, locations, ages, incomes, or many other things. Edges may represent friendship, but they could also represent animosity, or professional acquaintance, or geographical proximity. They can carry weights, representing, say, how well two people know each other. They can also be directed, pointing in only one direction. Graphs composed of directed edges are themselves called directed graphs or sometimes digraphs, for short. A graph representing telephone calls or email messages between individuals would be directed, since each message goes in only one direction. Directed graphs can be either cyclic, meaning they contain closed loops of edges, or acyclic meaning they do not. Some networks, such as food webs, are approximately but not perfectly acyclic.
    
    One can also have hyperedges—edges that join more than two vertices together. Graphs containing such edges are called hypergraphs. Hyperedges could be used to indicate family ties in a social network for example --- $n$ individuals connected to each other by virtue of belonging to the same immediate family could be represented by an $n$-edge joining them. Graphs may also be naturally partitioned in various ways. We will see a number of examples in this review of bipartite graphs: graphs that contain vertices of two distinct types, with edges running only between unlike types. So-called affiliation networks in which people are joined together by common membership of groups take this form, the two types of vertices representing the people and the groups. Graphs may also evolve over time, with vertices or edges appearing or disappearing, or values defined on those vertices and edges changing. And there are many other levels of sophistication one can add. The study of networks is by no means a complete science yet, and many of the possibilities have yet to be explored in depth, but we will see examples of at least some of the variations described here in the work reviewed in this paper.
    
    The jargon of the study of networks is unfortunately confused by differing usages among investigators from different fields. To avoid (or at least reduce) confusion, we give in Table I a short glossary of terms as they are used in this paper.
    
  \subsection{Real world networks}
  
    In this section we look at what is known about the structure of networks of different types. Recent work on the mathematics of networks has been driven largely by observations of the properties of actual networks and attempts to model them, so network data are the obvious starting point for a review such as this. It also makes sense to examine simultaneously data from different kinds of networks. One of the principal thrusts of recent work in this area, inspired particularly by a groundbreaking 1998 paper by Watts and Strogatz [416], has been the comparative study of networks from different branches of science, with emphasis on properties that are common to many of them and the mathematical developments that mirror those properties. We here divide our summary into four loose categories of networks: social networks, information networks, technological networks and biological networks.
    
    \subsubsection{Social networks}
    
      A social network is a set of people or groups of people with some pattern of contacts or interactions between them [363, 409]. The patterns of friendships between individuals [296, 348], business relationships between companies [269, 286], and intermarriages between families [327] are all examples of networks that have been studied in the past.1 Of the academic disciplines the social sciences have the longest history of the substantial quantitative study of real-world networks [162, 363]. Of particular note among the early works on the subject are: Jacob Moreno's work in the 1920s and 30s on friendship patterns within small groups [296]; the so-called \textquote{southern women study} of Davis et al. [103], which focused on the social circles of women in an unnamed city in the American south in 1936; the study by Elton Mayo and colleagues of social networks of factory workers in the late 1930s in Chicago [357]; the mathematical models of Anatol Rapoport [346], who was one of the first theorists, perhaps the first, to stress the importance of the degree distribution in networks of all kinds, not just social networks; and the studies of friendship networks of school children by Rapoport and others [149, 348]. In more recent years, studies of business communities [167, 168, 269] and of patterns of sexual contacts [45, 218, 243, 266, 303, 342] have attracted particular attention.
Another important set of experiments are the famous \textquote{small-world} experiments of Milgram [283, 393]. No actual networks were reconstructed in these experiments, but nonetheless they tell us about network structure. The experiments probed the distribution of path lengths in an acquaintance network by asking participants to pass a letter2 to one of their first-name acquaintances in an attempt to get it to an assigned target individual. Most of the letters in the experiment were lost, but about a quarter reached the target and passed on average through the hands of only about six people in doing so. This experiment was the origin of the popular concept of the \textquote{six degrees of separation,} although that phrase did not appear in Milgram's writing, being coined some decades later by Guare [183]. A brief but useful early review of Milgram’s work and work stemming from it was given by Garfield [169].
      
      Traditional social network studies often suffer from problems of inaccuracy, subjectivity, and small sample size. With the exception of a few ingenious indirect studies such as Milgram’s, data collection is usually carried out by querying participants directly using questionnaires or interviews. Such methods are labor-intensive and therefore limit the size of the network that can be observed. Survey data are, moreover, influenced by subjective biases on the part of respondents; how one respondent defines a friend for example could be quite different from how another does. Although much effort is put into eliminating possible sources of inconsistency, it is generally accepted that there are large and essentially uncontrolled errors in most of these studies. A review of the issues has been given by Marsden [271].
      
      Because of these problems many researchers have turned to other methods for probing social networks. One source of copious and relatively reliable data is collaboration networks. These are typically affiliation networks in which participants collaborate in groups of one kind or another, and links between pairs of individuals are established by common group membership. A classic, though rather frivolous, example of such a network is the collaboration network of film actors, which is thoroughly documented in the online Internet Movie Database.3 In this network actors collaborate in films and two actors are considered connected if they have appeared in a film together. Statistical properties of this network have been analyzed by a number of authors [4, 20, 323, 416]. Other examples of networks of this type are networks of company directors, in which two directors are linked if they belong to the same board of directors [104, 105, 269], networks of coauthorship among academics, in which individuals are linked if they have coauthored one or more papers [36, 43, 68, 107, 182, 279, 292, 311–313], and coappearance networks in which individuals are linked by mention in the same context, particularly on Web pages [3, 227] or in newspaper articles [99] (see Fig. 2b). Another source of reliable data about personal connections between people is communication records of certain kinds. For example, one could construct a network in which each (directed) edge between two people represented a letter or package sent by mail from one to the other. No study of such a network has been published as far as we are aware, but some similar things have. Aiello et al. [8, 9] have analyzed a network of telephone calls made over the AT\&T long-distance network on a single day. The vertices of this network represent telephone numbers and the directed edges calls from one number to another. Even for just a single day this graph is enormous, having about 50 million vertices, one of the largest graphs yet studied after the graph of the World Wide Web. Ebel et al. [136] have reconstructed the pattern of email communications between five thousand students at Kiel University from logs maintained by email servers. In this network the vertices represent email addresses and directed edges represent a message passing from one address to another. Email networks have also been studied by Newman et al. [321] and by Guimer'a et al. [185], and similar networks have been constructed for an \textquote{instant messaging} system by Smith [371], and for an Internet community Web site by Holme et al. [196]. Dodds et al. [110] have carried out an email version of Milgram's small-world experiment in which participants were asked to forward an email message to one of their friends in an effort to get the message ultimately to some chosen target individual. Response rates for the experiment were quite low, but a few hundred completed chains of messages were recorded, enough to allow various statistical analyses.
      
    \subsubsection{Information networks}
      
      Our second network category is what we will call information networks (also sometimes called \textquote{knowledge networks}). The classic example of an information network is the network of citations between academic papers [138]. Most learned articles cite previous work by others on related topics. These citations form a network in which the vertices are articles and a directed edge from article A to article B indicates that A cites B. The structure of the citation network then reflects the structure of the information stored at its vertices, hence the term \textquote{information network,} although certainly there are social aspects to the citation patterns of papers too [420].
Citation networks are acyclic (see Sec. I.A) because papers can only cite other papers that have already been written, not those that have yet to be written. Thus all edges in the network point backwards in time, making closed loops impossible, or at least extremely rare (see Fig. 4).
As an object of scientific study, citation networks have a great advantage in the copious and accurate data available for them. Quantitative study of publication patterns stretches back at least as far as Alfred Lotka's groundbreaking 1926 discovery of the so-called Law of Scientific Productivity, which states that the distribution of the numbers of papers written by individual scientists follows a power law. That is, the number of scientists who have written k papers falls off as $k−\alpha$ for some constant $\alpha$. (In fact, this result extends to the arts and humanities as well.) The first serious work on citation patterns was conducted in the 1960s as large citation databases became available through the work of Eugene Garfield and other pioneers in the field of bibliometrics. The network formed by citations was discussed in an early paper by Price [343], in which among other things, the author points out for the first time that both the in- and out-degree distributions of the network follow power laws, a far-reaching discovery which we discuss further in Sec. III.C. Many other studies of citation networks have been performed since then, using the ever better resources available in citation databases. Of particular note are the studies by Seglen [364] and Redner [351].4
        
      Another very important example of an information network is the World Wide Web, which is a network of Web pages containing information, linked together by hyperlinks from one page to another [203]. The Web should not be confused with the Internet, which is a physical network of computers linked together by optical fibre and other data connections.5 Unlike a citation network, the World Wide Web is cyclic; there is no natural ordering of sites and no constraints that prevent the appearance of closed loops (Fig. 4). The Web has been very heavily studied since its first appearance in the early 1990s, with the studies by Albert et al. [14, 34], Kleinberg et al. [241], and Broder et al. [74] being particularly influential. The Web also appears to have power-law in- and out-degree distributions (Sec. III.C), as well as a variety of other interesting properties [2, 14, 74, 158, 241, 254].
        
      One important point to notice about the Web is that our data about it come from \textquote{crawls} of the network, in which Web pages are found by following hyperlinks from other pages [74]. Our picture of the network structure of the World Wide Web is therefore necessarily biased. A page will only be found if another page points to it,6 and in a crawl that covers only a part of the Web (as all crawls do at present) pages are more likely to be found the more other pages point to them [263]. This suggests for instance that our measurements of the fraction of pages with low in-degree might be an underestimate.7 This behavior contrasts with that of a citation network. A paper can appear in the citation indices even if it has never been cited (and in fact a plurality of papers in the indices are never cited).
        
      A few other examples of information networks have been studied to a lesser extent. Jaffe and Trajtenberg [207], for instance, have studied the network of citations between US patents, which is similar in some respects to citations between academic papers. A number of authors have looked at peer-to-peer networks [5, 6, 205], which are virtual networks of computers that allow sharing of files between computer users over local- or wide-area networks. The network of relations be- tween word classes in a thesaurus has been studied by Knuth [244] and more recently by various other authors [234, 304, 384]. This network can be looked upon as an information network—users of a thesaurus \textquote{surf} the network from one word to another looking for the particular word that perfectly captures the idea they have in mind. However, it can also be looked at as a conceptual network representing the structure of the language, or possibly even the mental constructs used to represent the language. A number of other semantic word networks have also been investigated [119, 157, 369, 384].
        
      Preference networks provide an example of a bipartite information network. A preference network is a network with two kinds of vertices representing individuals and the objects of their preference, such as books or films, with an edge connecting each individual to the books or films they like. (Preference networks can also be weighted to indicate strength of likes or dislikes.) A widely studied example of a preference network is the EachMovie database of film preferences.8 Networks of this kind form the basis for collaborative filtering algorithms and recommender systems, which are techniques for predicting new likes or dislikes based on comparison of individuals' preferences with those of others [176, 352, 367]. Collaborative filtering has found considerable commercial success for product recommendation and targeted advertising, particularly with online retailers. Preference networks can also be thought of as social networks, linking not only people to objects, but also people to other people with similar preferences. This approach has been adopted occasionally in the literature [227].
        
    \subsubsection{Technological networks}
    
      Our third class of networks is technological networks, man-made networks designed typically for distribution of some commodity or resource, such as electricity or information. The electric power grid is a good example. This is a network of high-voltage three-phase transmission lines that spans a country or a portion of a country (as opposed to the local low-voltage a.c. power delivery lines that span individual neighborhoods). Statistical studies of power grids have been made by, for example, Watts and Strogatz [412, 416] and Amaral et al. [20]. Other distribution networks that have been studied include the network of airline routes [20], and networks of roads [221], railways [262, 366] and pedestrian traffic [87]. River networks could be regarded as a naturally occurring form of distribution network (actually a collection network) [111, 270, 353, 356], as could the vascular networks discussed in Sec. II.D. The telephone network and delivery networks such as those used by the post-office or parcel delivery companies also fall into this general category and are presumably studied within the relevant corporations, if not yet by academic researchers. (We distinguish here between the physical telephone network of wires and cables and the network of who calls whom, discussed in Sec. II.A.) Electronic circuits [155] fall somewhere between distribution and communication networks.
      
      Another very widely studied technological network is the Internet, i.e., the network of physical connections between computers. Since there is a large and ever-changing number of computers on the Internet, the structure of the network is usually examined at a coarse-grained level, either the level of routers, special-purpose computers on the network that control the movement of data, or \textquote{autonomous systems,} which are groups of computers within which networking is handled locally, but between which data flows over the public Internet. The computers at a single company or university would probably form a single autonomous system—autonomous systems often correspond roughly with domain names.
      
      In fact, the network of physical connections on the Internet is not easy to discover since the infrastructure is maintained by many separate organizations. Typically therefore, researchers reconstruct the network by reasoning from large samples of point-to-point data routes. So-called \textquote{traceroute} programs can report the sequence of network nodes that a data packet passes through when traveling between two points and if we assume an edge in the network between any two consecutive nodes along such a path then a sufficiently large sample of paths will give us a fairly complete picture of the entire network. There may however be some edges that never get sam- pled, so the reconstruction is typically a good, but not perfect, representation of the true physical structure of the Internet. Studies of Internet structure have been carried out by, among others, Faloutsos et al. [148], Broida and Claffy [75] and Chen et al. [86].
      
    \subsubsection{Biological networks}
    
      A number of biological systems can be usefully represented as networks. Perhaps the classic example of a biological network is the network of metabolic pathways, which is a representation of metabolic substrates and products with directed edges joining them if a known metabolic reaction exists that acts on a given substrate and produces a given product. Most of us will probably have seen at some point the giant maps of metabolic pathways that many molecular biologists pin to their walls.9 Studies of the statistical properties of metabolic networks have been performed by, for example, Jeong et al. [214, 340], Fell and Wagner [153, 405], and Stelling et al. [383]. A separate network is the network of mechanistic physical interactions between proteins (as opposed to chemical reactions among metabolites), which is usually referred to as a protein interaction network. Interaction networks have been studied by a number of authors [206, 212, 274, 376, 394].
      
      Another important class of biological network is the genetic regulatory network. The expression of a gene, i.e., the production by transcription and translation of the protein for which the gene codes, can be controlled by the presence of other proteins, both activators and inhibitors, so that the genome itself forms a switching net- work with vertices representing the proteins and directed edges representing dependence of protein production on the proteins at other vertices. The statistical structure of regulatory networks has been studied recently by various authors [152, 184, 368]. Genetic regulatory networks were in fact one of the first networked dynamical systems for which large-scale modeling attempts were made. The early work on random Boolean nets by Kauffman [224– 226] is a classic in this field, and anticipated recent developments by several decades.
      
      Another much studied example of a biological network is the food web, in which the vertices represent species in an ecosystem and a directed edge from species A to species B indicates that A preys on B [91, 339]—see Fig. 2a. (Sometimes the relationship is drawn the other way around, because ecologists tend to think in terms of energy or carbon flows through food webs; a predator- prey interaction is thus drawn as an arrow pointing from prey to predator, indicating energy flow from prey to predator when the prey is eaten.) Construction of complete food webs is a laborious business, but a number of quite extensive data sets have become available in recent years [27, 177, 204, 272]. Statistical studies of the topologies of food webs have been carried out by Sol ́e and Montoya [290, 375], Camacho et al. [82] and Dunne et al. [132, 133, 423], among others. A particularly thorough study of webs of plants and herbivores has been conducted by Jordano et al. [219], which includes statistics for no less than 53 different networks.
      
      Neural networks are another class of biological net- works of considerable importance. Measuring the topology of real neural networks is extremely difficult, but has been done successfully in a few cases. The best known example is the reconstruction of the 282-neuron neural network of the nematode C. Elegans by White et al. [421]. The network structure of the brain at larger scales than individual neurons—functional areas and pathways—has been investigated by Sporns et al. [379, 380].
      
      Blood vessels and the equivalent vascular networks in plants form the foundation for one of the most successful theoretical models of the effects of network structure on the behavior of a networked system, the theory of biological allometry [29, 417, 418], although we are not aware of any quantitative studies of their statistical structure.
      
      Finally we mention two examples of networks from the physical sciences, the network of free energy minima and saddle points in glasses [130] and the network of conformations of polymers and the transitions between them [361], both of which appear to have some interest- ing structural properties.
      
  \subsection{Power law}
    
  \subsection{Networks properties}
    
    \subsubsection{Small-world effect}
      
      The \emph{small-world experiment} comprised several experiments conducted by Stanley Milgram and other researchers examining the average path length for social networks of people in the United States. The research was groundbreaking in that it suggested that human society is a small-world-type network characterised by short path-lengths. The experiments are often associated with the phrase \textquote{six degrees of separation}, although Milgram did not use this term himself.
      
    \subsubsection{Transitivity and clustering coefficient}
    
      In networks it is quite common to find that if vertex A is is connected to vertex B and vertex B is connected to vertex C, then the probability that vertex A is connected to vertex C is higher than the probability that vertex A is connected to any other randomly chosen vertex. For example, in social networks, the friends of your friends are more likely to also be your friends than a random person that is not a friend of your friend. This deviation from the behaviour of random graphs can be seen in a network property called \emph{transitivity} (also known as \emph{clustering}, although this term has another meaning which may be confusing). From topological point of view, transitivity means that there are more sets of three vertices, each of which is connected to the other two -- called triangles -- in the network is heightened. It may be quantified by defining a \emph{clustering coefficient} $C$ thus:
      \begin{equation}
        C = \frac{3 \times \mbox{number of triangles in the network}}{\mbox{number of connected triples of vertices}} = \frac{\mbox{number of closed triplets}}{\mbox{number of connected triples}} \mbox{,}
      \end{equation}
      where a \textquote{connected triple} means a single vertex with edges running to an unordered pair of others.

      In graph theory, a clustering coefficient is a measure of degree to which nodes in a graph tend to cluster together. Evidence suggests that in most real-world networks, and in particular social networks, nodes tend to create tightly knit groups characterised by a relatively high density of ties; this likelihood tends to be greater than the average probability of a tie randomly established between two nodes\cite{HollandLeinhardt1971,WattsStrogatz1998}.
           
      Two versions of this measure exist: the global and the local. The global version was designed to give an overall indication of the clustering in the network, whereas the local gives an indication of the embeddedness of single nodes.
          
      \paragraph{Global clustering coefficient}
            
        The global clustering coefficient is based on triplets of nodes. A triplet consists of three nodes that are connected by either two (open triplet) or three (closed triplet) undirected ties. A triangle consists of three closed triplets, one centred on each of the nodes. The global clustering coefficient is the number of closed triplets (or $3 \times \mbox{triangles}$) over the total number of triplets (both open and closed). The first attempt to measure it was made by Luce and Perry (1949)\cite{LucePerry1949}. This measure gives an indication of the clustering in the whole network (global), and can be applied to both undirected and directed networks (often called transitivity, see Wasserman and Faust, 1994, page 243\cite{WassermanFaust1994}).
              
        Watts and Strogatz defined the clustering coefficient as follows, \textquote{Suppose that a vertex $v$ has $k_v$ neighbours; then at most $k_v(k_v - 1)/2$ edges can exist between them (this occurs when every neighbour of $v$ is connected to every other neighbour of $v$). Let $C_v$ denote the fraction of these allowable edges that actually exist. Define $C$ as the average of $C_v$ over all $v$.}
              
      \paragraph{Local clustering coefficient}
              
        The \emph{local clustering coefficient} of a vertex (node) in a graph quantifies how close its neighbours are to being a clique (complete graph). Duncan J. Watts and Steven Strogatz introduced the measure in 1998 to determine whether a graph is a small-world network.
              
        A graph $G=(V,E)$ formally consists of a set of vertices $V$ and a set of edges $E$ between them. An edge $e_{ij}$ connects vertex $v_i$ with vertex $v_j$.

        The neighbourhood $N_i$ for a vertex $v_i$ is defined as its immediately connected neighbours as follows:
        \begin{equation}
          N_i = \left\{v_j: e_{ij} \in E \wedge e_{ji} \in E\right\} \mbox{.}
        \end{equation}
        We define $k_i$ as the number of vertices, $|N_i|$, in the neighbourhood, $N_i$, of a vertex.

        The local clustering coefficient $C_i$ for a vertex $v_i$ is then given by the proportion of links between the vertices within its neighbourhood divided by the number of links that could possibly exist between them. For a directed graph, $e_{ij}$ is distinct from $e_{ji}$, and therefore for each neighbourhood $N_i$ there are $k_i(k_i-1)$ links that could exist among the vertices within the neighbourhood ($k_i$ is the number of neighbours of a vertex). Thus, the local clustering coefficient for directed graphs is given as \cite{WattsStrogatz1998}
        \begin{equation}
          C_i = \frac{\|\{e_{jk}: v_j,v_k \in N_i, e_{jk} \in E\}\|}{k_i(k_i-1)} \mbox{.}
        \end{equation}
    
        An undirected graph has the property that $e_{ij}$ and $e_{ji}$ are considered identical. Therefore, if a vertex $v_i$ has $k_i$ neighbours, $\frac{k_i(k_i-1)}{2}$ edges could exist among the vertices within the neighbourhood. Thus, the local clustering coefficient for undirected graphs can be defined as
        \begin{equation}
          C_i = \frac{2\|\{e_{jk}: v_j,v_k \in N_i, e_{jk} \in E\}\|}{k_i(k_i-1)} \mbox{.}
        \end{equation}

        Let $\lambda_G(v)$ be the number of triangles on $v \in V(G)$ for undirected graph $G$. That is, $\lambda_G(v)$ is the number of subgraphs of $G$ with 3 edges and 3 vertices, one of which is $v$. Let $\tau_G(v)$ be the number of triples on $v \in G$. That is, $\tau_G(v)$ is the number of subgraphs (not necessarily induced) with 2 edges and 3 vertices, one of which is $v$ and such that $v$ is incident to both edges. Then we can also define the clustering coefficient as
        \begin{equation}
          C_i = \frac{\lambda_G(v)}{\tau_G(v)} \mbox{.}
        \end{equation}

        It is simple to show that the two preceding definitions are the same, since
        \begin{equation}
          \tau_G(v) = C({k_i},2) = \frac{1}{2}k_i(k_i-1) \mbox{.}
        \end{equation}

        These measures are 1 if every neighbour connected to $v_i$ is also connected to every other vertex within the neighbourhood, and 0 if no vertex that is connected to $v_i$ connects to any other vertex that is connected to $v_i$.

    \subsubsection{Betweenness centrality}
        
      Betweenness centrality is a measure of a node's centrality in a network. It is equal to the number of shortest paths from all vertices to all others that pass through that node. Betweenness centrality is a more useful measure (than just connectivity) of both the load and importance of a node. The former is more global to the network, whereas the latter is only a local effect. Development of betweenness centrality is generally attributed to sociologist Linton Freeman, who has also developed a number of other centrality measures\cite{Freeman1977}.
          
      The betweenness centrality of a node $v$ is given by the expression:
      \begin{equation}
        g(v) = \sum_{s \neq v \neq t} \frac{\sigma_{st}(v)}{\sigma_{st}} \mbox{,}
      \end{equation}
      where $\sigma_{st}$ is the total number of shortest paths from node $s$ to node $t$ and $\sigma_{st}(v)$ is the number of those paths that pass through $v$.
          
      Note that the betweenness centrality of a node scales with the number of pairs of nodes as implied by the summation indices. Therefore the calculation may be rescaled by dividing through by the number of pairs of nodes not including $v$, so that $g \in [0,1]$. The division is done by $(N-1)(N-2)$ for directed graphs and $(N-1)(N-2)/2$ for undirected graphs, where $N$ is the number of nodes in the giant component. Note that this scales for the highest possible value, where one node is crossed by every single shortest path. This is often not the case, and a normalisation can be performed without a loss of precision
      \begin{equation}
        \mbox{normal}(g(v)) = \frac{g(v) - \min(g)}{\max(g) - \min(g)}\mbox{,}
      \end{equation}
      which results in:
      \begin{equation}
        \begin{split}
          \max(normal) = 1 \mbox{,}\\
          \min(normal) = 0 \mbox{.}
        \end{split}
      \end{equation}
      
      Note that this will always be a scaling from a smaller range into a larger range, so no precision is lost.
      
    \subsubsection{Degree distribution}
    
      In the study of graphs and networks, the degree of a node in a network is the number of connections it has to other nodes and the \emph{degree distribution} is the probability distribution of these degrees over the whole network.
      
      The degree of a node in a network (sometimes referred to incorrectly as the connectivity) is the number of connections or edges the node has to other nodes. If a network is directed, meaning that edges point in one direction from one node to another node, then nodes have two different degrees, the in-degree, which is the number of incoming edges, and the out-degree, which is the number of outgoing edges.

      The degree distribution $P(k)$ of a network is then defined to be the fraction of nodes in the network with degree $k$. Thus if there are $n$ nodes in total in a network and $nk$ of them have degree $k$, we have $P(k) = nk/n$.

      The same information is also sometimes presented in the form of a cumulative degree distribution, the fraction of nodes with degree greater than or equal to $k$.
      
      The degree distribution is very important in studying both real networks, such as the Internet and social networks, and theoretical networks. The simplest network model, for example, the (Bernoulli) random graph, in which each of $n$ nodes is connected (or not) with independent probability $p$ (or $1 − p$), has a binomial distribution of degrees:
      \begin{equation}
        P(k) = \binom{n-1}{k} p^k (1 - p)^{n-1-k}\mbox{,}
      \end{equation}
      (or Poisson in the limit of large $n$). Most networks in the real world, however, have degree distributions very different from this. Most are highly right-skewed, meaning that a large majority of nodes have low degree but a small number, known as \textquote{hubs}, have high degree. Some networks, notably the Internet, the world wide web, and some social networks are found to have degree distributions that approximately follow a power law: $P(k) \sim k−\gamma$, where $\gamma$ is a constant. Such networks are called scale-free networks and have attracted particular attention for their structural and dynamical properties.
      
      \paragraph{Scale-free networks}
  
        A scale-free network is a network whose degree distribution follows a power law, at least asymptotically. That is, the fraction $P(k)$ of nodes in the network having $k$ connections to other nodes goes for large values of $k$ as
    \begin{equation}
      P(k) \sim k^{-\gamma} \mbox{,}
    \end{equation}
    where $\gamma$ is a parameter whose value is typically in the range $2 < \gamma < 3$, although occasionally it may lie outside these bounds.\cite{OnnelaSaramakiBarabasi2007,ChoromanskiMatuszakMiekisz2013}

        Many networks are conjectured to be scale-free, including World Wide Web links, biological networks, and social networks, although the scientific community is still discussing these claims as more sophisticated data analysis techniques become available.\cite{ClausetShaliziNewman2007} Preferential attachment and the fitness model have been proposed as mechanisms to explain conjectured power law degree distributions in real networks.
        
      \paragraph{Maximum degree}
      
        The maximum degree $k_{\mbox{max}}$ of a vertex in a network will in general depend on the size of the network. For some calculations on networks the value of this maximum degree matters. In work on scale-free networks, Aiello et al. \cite{AielloChungLu2000} assumed that the maximum degree was approximately the value above which there is less than one vertex of that degree in the graph on average, i.e., the point where $np_k = 1$. This means, for instance, that $k_{\mbox{max}} \sim n^{1/\alpha}$ for the power-law degree distribution $p_k \sim k^{−\alpha}$. This assumption however can give misleading results; in many cases there will be vertices in the network with significantly higher degree than this, as discussed by Adamic et al.\cite{AdamicLukosePuniyaniHuberman2001}
        
        Given a particular degree distribution (and assuming all degrees to be sampled independently from it, which may not be true for networks in the real world), the probability of there being exactly $m$ vertices of degree $k$ and no vertices of higher degree is $\binom{n}{m} p^m_k (1 − P_k)^{n−m}$, where $P_k$ is the cumulative probability distribution. Hence the probability $h_k$ that the highest degree on the graph is $k$ is
        \begin{equation}
          h_k = \sum_{m=1}^n \binom{n}{m} p^m_k (1 - P_k)^{n-m} = (p_k + 1 - P_k)^n - (1 - P_k)^n\mbox{,}
        \end{equation}
        and the expected value of the highest degree is $k_{\mbox{max}} = \sum_k kh_k$.
        
        For both small and large values of $k$, $h_k$ tends to zero, and the sum over $k$ is dominated by the terms close to the maximum. Thus, in most cases, a good approximation to the expected value of the maximum degree is given by the modal value. Differentiating and observing that $\frac{\mbox{d}Pk}{\mbox{d}k} = p_k$, we find that the maximum of $h_k$ occurs when
        \begin{equation}
          \left(\frac{\mbox{d}Pk}{\mbox{d}k} - p_k\right)(p_k + 1 - P_k)^{n-1} p_k(1 - P_k)^{n-1} = 0\mbox{,}
        \end{equation}
        or $k_{\mbox{max}}$ is a solution of
        \begin{equation}
          \frac{\mbox{d}Pk}{\mbox{d}k} \simeq -np^2_k\mbox{,}
        \end{equation}
        where we have made the (fairly safe) assumption that $p_k$ is sufficiently small for $k \gtrsim k_{\mbox{max}}$ that $np_k \ll 1$ and $P_k \ll 1$.
        
        For example, if $p_k \sim k^{−\alpha}$ in its tail, then we find that
        \begin{equation}
          k_{\mbox{max}} \sim n^{1/(\alpha-1)}\mbox{.} \label{eqn:kmaxsim}
        \end{equation}
        
        As shown by Cohen et al. \cite{CohenErezAvrahamHavlin2000}, a simple rule of thumb that leads to the same result is that the maximum degree is roughly the value of $k$ that solves $nP_k = 1$. Note however that, as shown by Dorogovtsev and Samukhin\cite{DorogovtsevSamukhin2002}, the fluctuations in the tail of the degree distribution are very large for the power-law case. Dorogovtsev et al. \cite{DorogovtsevMendesSamukhin2001} have also shown that equation \ref{eqn:kmaxsim} holds for networks generated using the \textquote{preferential attachment} procedure of Barabási and Albert\cite{BarabasiAlbert1999}, and a detailed numerical study of this case has been carried out by Moreira et al. \cite{MoreiraAndradeAmaral2002}.
        
    \subsubsection{Resilience}
    
      Related to degree distributions is the property of \emph{resilience} of networks to the removal of their vertices, which has been the subject of a good deal of attention in the literature. Most of the networks we have been considering rely for their function on their connectivity, i.e., the existence of paths leading between pairs of vertices. If vertices are removed from a network, the typical length of these paths will increase, and ultimately vertex pairs will become disconnected and communication between them through the network will become impossible. Networks vary in their level of resilience to such vertex removal.
      
      There are also a variety of different ways in which vertices can be removed and different networks show varying degrees of resilience to these also. For example, one could remove vertices at random from a network, or one could target some specific class of vertices, such as those with the highest degrees. Network resilience is of particular importance in epidemiology, where \textquote{removal} of vertices in a contact network might correspond for example to vaccination of individuals against a disease. Because vaccination not only prevents the vaccinated individuals from catching the disease but may also destroy paths be- tween other individuals by which the disease might have spread, it can have a wider reaching effect than one might at first think, and careful consideration of the efficacy of different vaccination strategies could lead to substantial advantages for public health.
      
      Recent interest in network resilience has been sparked by the work of Albert et al. [15], who studied the effect of vertex deletion in two example networks, a 6000-vertex network representing the topology of the Internet at the level of autonomous systems, and a 326 000-page subset of the World Wide Web. Both of the Internet and the Web have been observed to have degree distributions that are approximately power-law in form [14, 74, 86, 148, 401]. The authors measured average vertex–vertex distances as a function of number of vertices removed, both for random removal and for progressive removal of the vertices with the highest degrees.\footnote{In removing the vertices with the highest degrees, Albert et al. recalculated degrees following the removal of each vertex. Most other authors who have studied this issue have adopted a slightly different strategy of removing vertices in order of their initial degree in the network before any removal.} They found for both networks that distance was almost entirely unaffected by random vertex removal, i.e., the networks studied were highly resilient to this type of removal. This is intuitively reasonable, since most of the vertices in these networks have low degree and therefore lie on few paths between others; thus their removal rarely affects communications substantially. On the other hand, when removal is targeted at the highest degree vertices, it is found to have devastating effect. Mean vertex–vertex distance increases very sharply with the fraction of vertices removed, and typically only a few percent of vertices need be removed before essentially all communication through the network is destroyed. Albert et al. expressed their results in terms of failure or sabotage of network nodes. The Internet (and the Web) they suggest, is highly resilient against the random failure of vertices in the network, but highly vulnerable to deliberate attack on its highest-degree vertices.
      
      Similar results to those of Albert et al. were found in- dependently by Broder et al. [74] for a much larger subset of the Web graph. Interestingly, however, Broder et al. gave an entirely opposite interpretation of their results. They found that in order to destroy connectivity in the Web one has to remove all vertices with degree greater than five, which seems like a drastic attack on the network, given that some vertices have degrees in the thousands. They thus concluded that the network was very resilient against targeted attack. In fact however there is not such a conflict between these results as at first appears. Because of the highly skewed degree distribution of the Web, the fraction of vertices with degree greater than five is only a small fraction of all vertices.
      
      Following these studies, many authors have looked into the question of resilience for other networks. In general the picture seems to be consistent with that seen in the Internet and Web. Most networks are robust against random vertex removal but considerably less robust to targeted removal of the highest-degree vertices. Jeong et al. [212] have looked at metabolic networks, Dunne et al. [132, 133] at food webs, Newman et al. [321] at email networks, and a variety of authors at resilience of model networks [15, 81, 93, 94, 200], which we discuss in more detail in later sections of the review. A particularly thorough study of the resilience of both real-world and model networks has been conducted by Holme et al. [200], who looked not only at vertex removal but also at removal of edges, and considered some additional strategies for selecting vertices based on so-called \textquote{betweenness}.
      
    \subsubsection{Mixing patterns}
    
      Delving a little deeper into the statistics of network structure, one can ask about which vertices pair up with which others. In most kinds of networks there are at least a few different types of vertices, and the probabilities of connection between vertices often depends on types. For example, in a food web representing which species eat which in an ecosystem (Sec. II.D) one sees vertices representing plants, herbivores, and carnivores. Many edges link the plants and herbivores, and many more the herbivores and carnivores. But there are few edges linking herbivores to other herbivores, or carnivores to plants. For the Internet, Maslov et al. [275] have proposed that the structure of the network reflects the existence of three broad categories of nodes: high- level connectivity providers who run the Internet back- bone and trunk lines, consumers who are end users of Internet service, and ISPs who join the two. Again there are many links between end users and ISPs, and many between ISPs and backbone operators, but few between ISPs and other ISPs, or between backbone operators and end users.
      
      In social networks this kind of selective linking is called assortative mixing or homophily and has been widely studied, as it has also in epidemiology. (The term \textquote{assortative matching} is also seen in the ecology literature, particularly in reference to mate choice among animals.) 
      
      A classic example of assortative mixing in social networks is mixing by race. Among other things, the study of 1958 couples in the city of San Francisco, California, recorded the race (self-identified) of study participants in each couple. Participants appeared to draw their partners preferentially from those of their own race, and this is believed to be a common phenomenon in many social networks: we tend to associate preferentially with people who are similar to ourselves in some way.

      Assortative mixing can be quantified by an \textquote{assortativity coefficient,} which can be defined in a couple of different ways. Let $E_{ij}$ be the number of edges in a network that connect vertices of types $i$ and $j$, with $i,j = 1 \ldots N$, and let $E$ be the matrix with elements $E_{ij}$. We define a normalized mixing matrix by
      \begin{equation}
        e = \frac{E}{||E||}\mbox{,}
      \end{equation}
      where $||x||$ means the sum of all the elements of the matrix $x$. The elements $e_{ij}$ measure the fraction of edges that fall between vertices of types $i$ and $j$. One can also ask about the conditional probability $P(j|i)$ that my network neighbor is of type $j$ given that I am of type $i$, which is given by $P (j|i) = e_{ij} / \sum_j e_{ij}$. These quantities satisfy the normalization conditions
      \begin{equation}
        \sum_{ij} e_{ij} = 1\mbox{,} \sum_j P(j|i) = 1\mbox{.}
      \end{equation}
      
      Gupta et al. [186] have suggested that assortative mixing be quantified by the coefficient
      \begin{equation}
        Q = \frac{\sum_i P(i|i) - 1}{N - 1}\mbox{.}
      \end{equation}

      This quantity has the desirable properties that it is $1$ for a perfectly assortative network (every edge falls between vertices of the same type), and $0$ for randomly mixed networks, and it has been quite widely used in the literature. But it suffers from two shortcomings [318]: (1) for an asymmetric matrix, Q has two different values, depending on whether we put the men or the women along the horizontal axis, and it is unclear which of these two values is the \textquote{correct} one for the network; (2) the measure weights each vertex type equally, regardless of how many vertices there are of each type, which can give rise to misleading figures for $Q$ in cases where community size is heterogeneous, as it often is.
      
      An alternative assortativity coefficient that remedies these problems is defined by [318]
      \begin{equation}
        r = \frac{\mbox{Tr}e - ||e^2||}{1 - ||e^2||}\mbox{.}
      \end{equation}

      This quantity is also $0$ in a randomly mixed network and $1$ in a perfectly assortative one. But its value is not altered by transposition of the matrix and it weights vertices equally rather than communities, so that small communities make an appropriately small contribution to $r$.
      
      Another type of assortative mixing is mixing by scalar characteristics such as age or income. Again it is usually found that people prefer to associate with others of similar age and income to themselves, although of course age and income, like race, may be proxies for other driving forces, such as cultural differences. Garfinkel et al. [170] and Newman [318], for example, have analyzed data for unmarried and married couples respectively to show that there is strong correlation between the ages of partners. Mixing by scalar characteristics can be quantified by calculating a correlation coefficient for the characteristic in question.
      
      In theory assortative mixing according to vector characteristics should also be possible. For example, geographic location probably affects individuals' propensity to become acquainted. Location could be viewed as a two-vector, with the probability of connection between pairs of individuals being assortative on the values of these vectors.
            
    \subsubsection{Degree correlations}
    
      A special case of assortative mixing according to a scalar vertex property is mixing according to vertex degree, also commonly referred to simply as degree correlation. Do the high-degree vertices in a network associate preferentially with other high-degree vertices? Or do they prefer to attach to low-degree ones? Both situations are seen in some networks, as it turns out. The case of assortative mixing by degree is of particular interest because, since degree is itself a property of the graph topology, degree correlations can give rise to some interesting network structure effects.
      
      Several different ways of quantifying degree correlations have been proposed. Maslov et al. [274, 275] have simply plotted the two-dimensional histogram of the degrees of vertices at either ends of an edge. They have shown results for protein interaction networks and the Internet. A more compact representation of the situation is that proposed by Pastor-Satorras et al. [331, 401], who in studies of the Internet calculated the mean degree of the network neighbors of a vertex as a function of the degree $k$ of that vertex. This gives a one-parameter curve which increases with $k$ if the network is assortatively mixed. For the Internet in fact it is found to decrease with $k$, a situation we call disassortativity. Newman [314, 318] reduced the measurement still further to a single number by calculating the Pearson correlation coefficient of the degrees at either ends of an edge. This gives a single number that should be positive for assortatively mixed networks and negative for disassortative ones. An interesting observation is that essentially all social networks measured appear to be assortative, but other types of networks (information networks, technological networks, biological networks) appear to be disassortative. It is not clear what the explanation for this result is, or even if there is any one single explanation. (Probably there is not.)
    
    \subsubsection{Community structure}
    
      It is widely assumed [363, 409] that most social networks show \textquote{community structure,} i.e., groups of vertices that have a high density of edges within them, with a lower density of edges between groups. It is a matter of common experience that people do divide into groups along lines of interest, occupation, age, and so forth, and the phenomenon of assortativity certainly suggests that this might be the case. (It is possible for a network to have assortative mixing but no community structure. This can occur, for example, when there is assortative mixing by age or other scalar quantities. Networks with this type of structure are sometimes said to be \textquote{stratified.})
    
      One might well imagine for example that citation networks would divide into groups representing particular areas of research interest, and a good deal of energy has been invested in studies of this phenomenon [101, 138]. Similarly communities in the World Wide Web might reflect the subject matter of pages, communities in metabolic, neural, or software networks might reflect functional units, communities in food webs might reflect subsystems within ecosystems, and so on.
      
      The traditional method for extracting community structure from a network is cluster analysis [147], sometimes also called hierarchical clustering.\footnote{Not to be confused with the entirely different use of the word clustering} In this method, one assigns a \textquote{connection strength} to vertex pairs in the network of interest. In general each of the $\frac{1}{2} n(n − 1)$ possible pairs in a network of $n$ vertices is assigned such a strength, not just those that are connected by an edge, although there are versions of the method where not all pairs are assigned a strength; in that case one can assume the remaining pairs to have a connection strength of zero. Then, starting with n vertices with no edges between any of them, one adds edges in order of decreasing vertex–vertex connection strength. One can pause at any point in this process and examine the component structure formed by the edges added so far; these components are taken to be the communities (or \textquote{clusters}) at that stage in the process. When all edges have been added, all vertices are connected to all others, and there is only one community. The entire process can be represented by a tree or dendrogram of union operations between vertex sets in which the communities at any level correspond to a horizontal cut through the tree.
      
      Clustering is possible according to many different definitions of the connection strength. Reasonable choices include various weighted vertex–vertex distance measures, the sizes of minimum cut-sets (i.e., maximum flow) [7], and weighted path counts between vertices. Recently a number of authors have had success with methods based on \textquote{edge betweenness,} which is the count of how many geodesic paths between vertices run along each edge in the network [171, 185, 197, 422]. Results appear to show that, for social and biological networks at least, community structure is a common network property, although some food webs are found not to break up into communities in any simple way. (Food webs may be different from other networks in that they appear to be dense: mean vertex degree increases roughly linearly with net- work size, rather than remaining constant as it does in most networks [132, 273]. The same may be true of metabolic networks also [P. Holme, personal communication].)
  
    
      Network clustering should not be confused with the technique of data clustering, which is a way of detect- ing groupings of data-points in high-dimensional data spaces [208]. The two problems do have some common features however, and algorithms for one can be adapted for the other, and vice versa. For example, high-dimensional data can be converted into a network by placing edges between closely spaced data points, and then network clustering algorithms can be applied to the result. On balance, however, one normally finds that algorithms specially devised for data clustering work better than such borrowed methods, and the same is true in reverse.
  
    
      In the social networks literature, network clustering has been discussed to a great extent in the context of so-called block models, [71, 419] which are essentially just divisions of networks into communities or blocks accord- ing to one criterion or another. Sociologists have concentrated particularly on structural equivalence. Two vertices in a network are said to be structurally equivalent if they have all of the same neighbours. Exact structural equivalence is rare, but approximate equivalence can be used as the basis for a hierarchical clustering method such as that described above.
      
      Another slightly different question about community structure, but related to the one discussed here, has been studied by Flake et al. [158]: if one is given an example vertex drawn from a known network, can one identify the community to which it belongs? Algorithmic methods for answering this question would clearly be of some practical value for searching networks such as the World Wide Web and citation networks. Flake et al. give what appears to be a very successful algorithm, at least in the context of the Web, based on a maximum flow method.
      
    \subsubsection{Other properties}
    
      In addition to the heavily studied network properties of the preceding sections, a number of others have received some attention. In some networks the size of the largest component is an important quantity. For example, in a communication network like the Internet the size of the largest component represents the largest fraction of the network within which communication is possible and hence is a measure of the effectiveness of the network at doing its job [74, 81, 93, 94, 125, 323]. The size of the largest component is often equated with the graph theoretical concept of the \textquote{giant component}, although technically the two are only the same in the limit of large graph size. The size of the second-largest component in a network is also measured sometimes. In networks well above the density at which a giant component first forms, the largest component is expected to be much larger than the second largest.
      
      Goh et al. [175] have made a statistical study of the distribution of the \textquote{betweenness centrality} of vertices in networks. The betweenness centrality of a vertex $i$ is the number of geodesic paths between other vertices that run through $i$ [161, 363, 409]. Goh et al. show that betweenness appears to follow a power law for many networks and propose a classification of networks into two kinds based on the exponent of this power law. Betweenness centrality can also be viewed as a measure of network resilience [200, 312]—it tells us how many geodesic paths will get longer when a vertex is removed from the net- work. Latora and Marchiori [260, 261] have considered the harmonic mean distance between a vertex and all others, which they call the \textquote{efficiency} of the vertex. This, like betweenness centrality, can be viewed as a measure of network resilience, indicating how much effect on path length the removal of a vertex will have. A number of authors have looked at the eigenvalue spectra and eigenvectors of the graph Laplacian (or equivalently the adjacency matrix) of a network [55, 146, 151], which tells us about diffusion or vibration modes of the network, and about vertex centrality. [66, 67].
      
      Milo et al. [284, 368] have presented a novel analysis that picks out recurrent motifs—small subgraphs—from complete networks. They apply their method to genetic regulatory networks, food webs, neural networks and the World Wide Web, finding different motifs in each case. They have also made suggestions about the possible function of these motifs within the networks. In regulatory networks, for instance, they identify common subgraphs with particular switching functions in the system, such as gates and other feed-forward logical operations.

  \subsection{Network growth}
    
  \subsection{Processes taking place in networks}